---
title: "ISLR Model Select"
output: html_document
---

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.


```{r}
require(ISLR)
summary(Hitters)
```
Remove missing values
```{r}
Hitters=na.omit(Hitters)
# check for any na's
with(Hitters,sum(is.na(Salary)))
```

Best subset regression
---
use package `leaps` to identify best-subset models
```{r}
library(leaps)
regfit.full=regsubsets(Salary~.,data=Hitters)
summary(regfit.full)
```

increase subset count to 19, all the variable in this instance
```{r}
regfit.full=regsubsets(Salary~.,data=Hitters,nvmax=19)
reg.summary=summary(regfit.full)
names(reg.summary)
# plot 'Cp' to identify a model with lowest error
plot(reg.summary$cp,xlab="variable count",ylab="Cp")
which.min(reg.summary$cp)
points(10,reg.summary$cp[10],pch=20,col="red")
```

plot method for `regsubset` object
```{r}
plot(regfit.full, scale="Cp")
coef(regfit.full, 10)
```


Forward Stepwise Selection
---
use `regsubsets` function and specify `method="forward"`
```{r}
regfit.fwd=regsubsets(Salary~.,data=Hitters,nvmax=19,method="forward")
summary(regfit.fwd)
plot(regfit.fwd,scale="Cp")

```

Model Selection Using a Validation Set
---
```{r}
dim(Hitters)
set.seed(1)
# 2/3 training, 1/3 test
train=sample(seq(263),180,replace=FALSE)
train
regfit.fwd=regsubsets(Salary~.,data=Hitters[train,],nvmax=19, method="forward")
```
make predictions
19 variables/models, set up vectors
```{r}
val.errors=rep(NA,19)
x.test=model.matrix(Salary~.,data=Hitters[-train,])
for(i in 1:19){
    coefi=coef(regfit.fwd,id=i)
    # there is no predict method for regsubsets, thus:
    pred=x.test[,names(coefi)]%*%coefi
    val.errors[i]=mean((Hitters$Salary[-train]-pred)^2)
}
plot(sqrt(val.errors),ylab="Root MSE",ylim=c(300,400),pch=19,type="b")
points(sqrt(regfit.fwd$rss[-1]/180),col="blue",pch=19,type="b")
legend("topright", legend=c("Training", "Validation"), col=c("blue", "black"), pch=19)
```
above was tedious - not having a predict method for `regsubsets`
create one:
```{r}
predict.regsubsets=function(object,newdata,id,...){
    form=as.formula(object$call[[2]])
    mat=model.matrix(form,newdata)
    coefi=coef(object,id=id)
    mat[,names(coefi)]%*%coefi
}
```

Model Selection by Cross-Validation
---
10-fold CV, very easy
```{r}
set.seed(11)
folds=sample(rep(1:10,length=nrow(Hitters)))
folds
table(folds)
cv.errors=matrix(NA,10,19)

for(k in 1:10){
    best.fit=regsubsets(Salary~.,data=Hitters[folds!=k,],nvmax=19,method="forward")
    for(i in 1:19){
        pred=predict(best.fit,Hitters[folds==k,], id=i)
        cv.errors[k,i]=mean((Hitters$Salary[folds==k]-pred)^2)
    }
}
rmse.cv=sqrt(apply(cv.errors,2,mean))
plot(rmse.cv,pch=19,type="b")
```

Ridge Regression and the Lasso
---
Using `glmnet`
```{r}
library(glmnet)
x=model.matrix(Salary~.-1,data=Hitters)
y=Hitters$Salary
```

first fit a ridge-regression model
achieved by calling `glmnet` with `alpha=0`
```{r}
fit.ridge=glmnet(x,y,alpha=0)
plot(fit.ridge,xvar="lambda",label=TRUE)
# cross-validation
cv.ridge=cv.glmnet(x,y,alpha=0)
plot(cv.ridge)
```

fit a lasso model
use default of `alpha=1`
```{r}
fit.lasso=glmnet(x,y)
plot(fit.lasso,xvar="lambda",label=TRUE)
# view other performance, such as % deviance explained
plot(fit.lasso,xvar="dev",label=TRUE)
# cross-validation
cv.lasso=cv.glmnet(x,y)
plot(cv.lasso)
coef(cv.lasso)
```

Use previous train/validate to select `lambda` for lasso
```{r}
lasso.tr=glmnet(x[train,],y[train])
lasso.tr
pred=predict(lasso.tr,x[-train,])
dim(pred)
rmse=sqrt(apply((y[-train]-pred)^2,2,mean))
plot(log(lasso.tr$lambda),rmse,type="b",xlab="Log(lambda)")
lam.best=lasso.tr$lambda[order(rmse)[1]]
lam.best
coef(lasso.tr,s=lam.best)
```
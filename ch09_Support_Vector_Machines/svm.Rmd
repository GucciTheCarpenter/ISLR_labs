SVM
===

<br>
<br>
Demonstrate in low dimensions

Linear SVM classifier
---

Two dimensional data slightly separated.
```{r}
set.seed(10111)
x=matrix(rnorm(40),20,2)
y=rep(c(-1,1),c(10,10))
x[y==1,]=x[y==1,]+1
plot(x,col=y+3,pch=19)
```

load package `e1071` containing `svm` function (br)
will have to specify tuning parameter, `icost`
```{r}
library(e1071)
dat=data.frame(x,y=as.factor(y))
svmfit=svm(y~.,data=dat,kernel="linear",cost=10,scale=FALSE)
print(svmfit)
plot(svmfit,dat)
```

Plot function somewhat crude and plots X2 on the horizontal axis, unlike R's automatic matix plotting

Create function with `expand.grid`
```{r}
make.grid=function(x,n=75){
    grange=apply(x,2,range)
    x1=seq(from=grange[1,1],to=grange[2,1],length=n)
    x2=seq(from=grange[1,2],to=grange[2,2],length=n)
    expand.grid(X1=x1,X2=x2)
}
xgrid=make.grid(x)
ygrid=predict(svmfit,xgrid)
plot(xgrid,col=c("red","blue")[as.numeric(ygrid)],pch=20,cex=.2)
points(x,col=y+3,pch=19)
# highlight support vector (points)
points(x[svmfit$index,],pch=5,cex=2)
```

Extract linear coefficients<br>
Include decision boundary WITH the two margins
```{r}
beta=drop(t(svmfit$coefs)%*%x[svmfit$index,])
beta0=svmfit$rho
plot(xgrid,col=c("red","blue")[as.numeric(ygrid)],pch=20,cex=.2)
points(x,col=y+3,pch=19)
points(x[svmfit$index,],pch=5,cex=2)
# abline requires intercept and slop
abline(beta0/beta[2],-beta[1]/beta[2])
abline((beta0-1)/beta[2],-beta[1]/beta[2],lty=2)
abline((beta0+1)/beta[2],-beta[1]/beta[2],lty=2)
```

Nonlinear SVM
---

Mixture data from ESL
```{r}
load(url("http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/ESL.mixture.rda"))
names(ESL.mixture)
# remove previous x & y from above
rm(x,y)
attach(ESL.mixture)
```
Plot nonlinear SVM, using a radial kernel
```{r}
plot(x,col=y+1)
dat=data.frame(y=factor(y),x)
fit=svm(factor(y)~.,data=dat,scale=FALSE,kernel="radial",cost=5)
```
Make grid and make predictions on grid.
```{r}
### no need to use function, as data set comes with grid coordinates:
### px1 & px2
# xgrid=make.grid(x,100)
# ygrid=predict(fit,xgrid)
#plot(xgrid,col=c("red","blue")[as.numeric(ygrid)],pch=20,cex=.2)
#points(x,col=y+1)
xgrid = expand.grid(X1=px1,X2=px2)
ygrid=predict(fit,xgrid)
plot(xgrid,col=as.numeric(ygrid),pch=20,cex=.2)
points(x,col=y+1,pch=19)
```
Make use of contour function to plot decision boundary.<br>
`ESL.mixture` has `prob` with true class probability.<br>
If we plot its 0.5 contour, will get _Bayes Decision Boundary_
```{r}
func=predict(fit,xgrid,decision.values = T)
func=attributes(func)$decision
# recreate grid/plot if needed
xgrid = expand.grid(X1=px1,X2=px2)
ygrid=predict(fit,xgrid)
plot(xgrid,col=as.numeric(ygrid),pch=20,cex=.2)
points(x,col=y+1,pch=19)

contour(px1,px2,matrix(func,69,99),level=0,add=TRUE)
contour(px1,px2,matrix(prob,69,99),level=.5,add=T,col="blue")
```
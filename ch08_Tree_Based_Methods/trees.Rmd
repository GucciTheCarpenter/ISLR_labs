---
title: "Decision Trees"
output: html_document
---

```{r}
require(ISLR)
require(tree)
attach(Carseats)
hist(Sales)
High=ifelse(Sales<=8,"No","Yes")
Carseats=data.frame(Carseats, High)
```
__Exclude__ `Sales` from formula since `High` was derived from it.
```{r}
tree.carseats=tree(High~.-Sales,data=Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats,pretty=0)
```
Print detailed summary of tree:
```{r}
tree.carseats
```
Create training and test set with 250/150 split, respectively, of 400 observations.
```{r}
set.seed(1011)
train=sample(1:nrow(Carseats),250)
tree.carseats=tree(High~.-Sales,Carseats, subset=train)
plot(tree.carseats); text(tree.carseats,pretty=0)
tree.predict=predict(tree.carseats,Carseats[-train,],type="class")
with(Carseats[-train,],table(tree.predict,High))
# accuracy
(72+33)/150
```
To avoid overfitting we use CV to prune.
```{r}
# 10-fold CV
cv.carseats=cv.tree(tree.carseats,FUN = prune.misclass)
cv.carseats
plot(cv.carseats)
prune.carseats=prune.misclass(tree.carseats,best=13)
plot(prune.carseats);text(prune.carseats,pretty=0)
```
Re-evaluate
```{r}
tree.pred=predict(prune.carseats,Carseats[-train,],type="class")
with(Carseats[-train,],table(tree.pred,High))
(72+32)/150
```
Similar performance but with shallower/simpler tree. Good for interpretation and avoiding overfitting (memorization NOT genarlization).


Random Forests and Boosting
===
Will use Boston housing data st from `MASS` package.

Random Forests
---
Lots of non-pruned trees, average them to reduce variance.
```{r}
require(randomForest)
require(MASS)
set.seed(101)
dim(Boston)
train=sample(1:nrow(Boston),300)
?Boston
```
Response feature: `medv`

```{r}
rf.boston=randomForest(medv~.,data=Boston,subset=train)
rf.boston
```
No. of variables tried at each split: 4, `mtry=4`; we can try all 13 predictors.
```{r}
# out-of-bag
oob.err=double(13)
test.err=double(13)
for(mtry in 1:13){
    fit=randomForest(medv~.,data=Boston,subset=train,mtry=mtry,ntree=400)
    oob.err[mtry]=fit$mse[400]
    pred=predict(fit,Boston[-train,])
    test.err[mtry]=with(Boston[-train,],mean((medv-pred)^2))
    cat(mtry, " ")
}
matplot(1:mtry,cbind(test.err,oob.err),pch=19,col=c("red","blue"),type="b",ylab="Mean Square Error")
legend("topright",legend=c("OOB","Test"),pch=19,col=c("red","blue"))
```

Boosting
---
"Boosting builds lots of smaller trees. Unlike random forests, each new tree in boosting tries to patch up the deficiencies of the current ensemble."
```{r}
# Generalized Boosted Regression Modeling
require(gbm)
boost.boston=gbm(medv~.,data=Boston[train,],distribution = "gaussian",n.trees = 10000,shrinkage = 0.01,interaction.depth = 4)
summary(boost.boston)
# lstat - lower status of the population (percent).
# rm - average number of rooms per dwelling.
plot(boost.boston,i="lstat")
plot(boost.boston,i="rm")
```

TODO: use CV to determine # of trees in boosting
```{r}
n.trees=seq(from=100,to=10000,by=100)
predmat=predict(boost.boston,newdata = Boston[-train,],n.trees = n.trees)
dim(predmat)
berr=with(Boston[-train,],apply( (predmat-medv)^2,2,mean))
plot(n.trees,berr,pch=19, ylab = "Mean Square Error", xlab = "# Trees", main = "Boosting Test Error")
# overlay best test error from randomForest
abline(h=min(test.err),col="red")
```